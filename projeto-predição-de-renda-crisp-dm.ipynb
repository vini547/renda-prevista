{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão de renda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 elementos importantes\n",
    "- Esse notebook\n",
    "- Streamlit com as análises\n",
    "- Seu Github com o projeto\n",
    "- Vídeo no readme do github mostrando o streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1 CRISP - DM: Entendimento do negócio\n",
    "\n",
    "Os bancos são instituições financeiras responsáveis por oferecer diversos serviços, como contas correntes, investimentos, empréstimos e financiamentos. Para operar de maneira eficiente e segura, eles precisam avaliar o perfil financeiro de seus clientes, garantindo que possam oferecer produtos adequados sem comprometer sua sustentabilidade.  \n",
    "Um dos principais desafios enfrentados pelos bancos é a avaliação da renda dos clientes. Essa informação é essencial para diversas operações, tais como:\n",
    "\n",
    "- **Concessão de crédito**: Antes de aprovar um empréstimo ou financiamento, o banco precisa estimar a capacidade de pagamento do cliente para evitar inadimplência.\n",
    "- **Definição de limites de cartões e cheque especial**: Bancos ajustam limites de crédito com base na renda estimada do cliente.\n",
    "- **Personalização de produtos financeiros**: Com uma previsão precisa de renda, é possível oferecer investimentos e serviços mais alinhados ao perfil do cliente.\n",
    "- **Compliance e regulamentação**: Autoridades regulatórias exigem que os bancos adotem práticas responsáveis na concessão de crédito, o que exige análises detalhadas da situação financeira dos clientes.\n",
    "- **Personalização de investimentos**: Com a previsão de renda, é possível oferecer opções de investimentos adequadas ao perfil financeiro do cliente, como fundos de baixo risco para quem tem uma renda mais restrita, ou alternativas de maior retorno para clientes com maior capacidade financeira.\n",
    "\n",
    "No entanto, nem sempre a informação de renda está disponível ou é declarada corretamente pelos clientes. Para contornar essa limitação, os bancos podem utilizar modelos de Machine Learning para prever a renda de um cliente com base em outras variáveis do seu perfil, como idade, histórico de crédito, comportamento financeiro e dados demográficos.\n",
    "\n",
    "Neste projeto, o objetivo é desenvolver um modelo preditivo para estimar a renda dos clientes com base em seus atributos, permitindo que o banco otimize sua tomada de decisão e melhore a experiência do usuário fornecendo uma ferramenta que ajudará o cliente a tomar crédito de forma correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2 Crisp-DM: Entendimento dos dados\n",
    "\n",
    "<span >Na etapa de Compreensão dos Dados do CRISP-DM, o foco está na coleta, análise e exploração inicial dos dados disponíveis. O objetivo principal é entender a estrutura, qualidade e relevância dos dados para o projeto. Aqui estão os passos detalhados para essa etapa, utilizando as bibliotecas mencionadas.\n",
    "\n",
    "1. Coleta e Identificação das Fontes de Dados: A coleta de dados pode ocorrer de diversas fontes, como SQL, Excel, APIs externas, entre outros. Os dados podem ser coletados em intervalos diários, mensais ou anuais, dependendo da necessidade do negócio. Alguns dados podem vir de servidores internos da empresa, enquanto outros podem ser coletados externamente, como taxas de inflação calculadas por fontes oficiais ou dados de fornecedores como SERASA ou Boa Vista. Dados de crawlers da web também podem ser utilizados para enriquecer a base de dados. Neste caso o csv foi fornecido. E para sua manipulação utilizaremos o Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             15000 non-null  int64  \n",
      " 1   data_ref               15000 non-null  object \n",
      " 2   id_cliente             15000 non-null  int64  \n",
      " 3   sexo                   15000 non-null  object \n",
      " 4   posse_de_veiculo       15000 non-null  bool   \n",
      " 5   posse_de_imovel        15000 non-null  bool   \n",
      " 6   qtd_filhos             15000 non-null  int64  \n",
      " 7   tipo_renda             15000 non-null  object \n",
      " 8   educacao               15000 non-null  object \n",
      " 9   estado_civil           15000 non-null  object \n",
      " 10  tipo_residencia        15000 non-null  object \n",
      " 11  idade                  15000 non-null  int64  \n",
      " 12  tempo_emprego          12427 non-null  float64\n",
      " 13  qt_pessoas_residencia  15000 non-null  float64\n",
      " 14  renda                  15000 non-null  float64\n",
      "dtypes: bool(2), float64(3), int64(4), object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./test/input/previsao_de_renda.csv\")  # Exemplo de carregamento de dados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removeremos as colunas: id_cliente, Unnamed: 0 e data_ref pois não fazem sentido na análise neste momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sexo                   15000 non-null  object \n",
      " 1   posse_de_veiculo       15000 non-null  bool   \n",
      " 2   posse_de_imovel        15000 non-null  bool   \n",
      " 3   qtd_filhos             15000 non-null  int64  \n",
      " 4   tipo_renda             15000 non-null  object \n",
      " 5   educacao               15000 non-null  object \n",
      " 6   estado_civil           15000 non-null  object \n",
      " 7   tipo_residencia        15000 non-null  object \n",
      " 8   idade                  15000 non-null  int64  \n",
      " 9   tempo_emprego          12427 non-null  float64\n",
      " 10  qt_pessoas_residencia  15000 non-null  int32  \n",
      " 11  renda                  15000 non-null  float64\n",
      "dtypes: bool(2), float64(2), int32(1), int64(2), object(5)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df['qt_pessoas_residencia'] = df['qt_pessoas_residencia'].astype(int)\n",
    "df.drop(columns=['id_cliente','Unnamed: 0','data_ref'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Análise Exploratória Inicial: Com os dados coletados, a primeira ação é carregá-los no ambiente de trabalho para uma análise preliminar. Usando pandas, pode-se importar os dados de diferentes formatos como CSV, Excel, ou bancos de dados SQL. O primeiro passo é realizar uma inspeção dos dados para identificar as variáveis disponíveis, seus tipos, e verificar a presença de valores ausentes ou inconsistentes. Para isso utilizaremos **ProfileReport** do **ydata_profiling**, O ydata-profiling é uma ferramenta de ponta na etapa de entendimento dos dados no fluxo de trabalho de ciência de dados, sendo um pacote pioneiro em Python para perfilagem de dados. O **ydata_profiling** é um pacote amplamente utilizado para perfilagem automática de dados, padronizando a geração de relatórios detalhados que incluem estatísticas descritivas e visualizações, facilitando a análise exploratória e a identificação de padrões, anomalias e correlações nas variáveis. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import warnings\n",
    "# Gerar relatório sem autocorrelação\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    explorative=True,\n",
    "    correlations={\"auto\": {\"calculate\": False}}  # <- isso desativa a autocorrelação\n",
    ")\n",
    "\n",
    "# Save the report as an HTML file\n",
    "profile.to_file(\"report.html\")\n",
    "\n",
    "# Or display in a Jupyter Notebook\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProfileReport revelou destaques do dataset através do seu Overview e seu Alerts:\n",
    "- **Variáveis**: Total de 12. 5 Categóricas,  2 Booleanas, 5 Numéricas.Também revelou 15000 observações de cada com 3460 duplicadas, o que pode ser maior considerendo todas as colunas do dataset.\n",
    "- **qtd_filhos**: tem 944 (6.3%) zeros.\n",
    "- **tempo_emprego**: tem 2573 (17.2%) valores faltantes.\n",
    "- **tipo_residencia**: tem um alto imbalance (75.1%).\n",
    "- **Consumo geral de memória**: 5,7mb, com tamanho médio de linha salva de 396,7 bytes.\n",
    "\n",
    "Estes resultados sugerem quais tipos de tratamento o dataset precisa para atingir a qualidade necessária para as próximas etapas do CRISP-DM como:\n",
    "- **Tratamento de Nan/Missing Values**\n",
    "- **Tratamento de Linhas Duplicadas**\n",
    "- **Imbalance**\n",
    "- **Normalização**\n",
    "- **Dummies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicionário de dados\n",
    "\n",
    "| Variável                | Descrição                                           | Tipo         | Natureza         |\n",
    "| ----------------------- |:---------------------------------------------------:| ------------:| ----------------:|\n",
    "| data_ref                |  Data da referência da análise                                      | Data| Temporal         |\n",
    "| id_cliente              |  Identificação única do cliente                                      | Inteiro| Discreta         |\n",
    "| sexo                    |  Sexo do cliente                                      | Object| Categórica         |\n",
    "| posse_de_veiculo        |  Se o cliente possui veículo                                      | Booleano| Categórica         |\n",
    "| posse_de_imovel         |  Se o cliente possui imóvel                                     | Booleano| Categórica         |\n",
    "| qtd_filhos              |  Número de filhos                                     | Inteiro| Discreta         |\n",
    "| tipo_renda              |  Tipo de fonte de renda                                    | Object| Categórica         |\n",
    "| educacao                |  Nível de educação                                     | Object| Categórica         |\n",
    "| estado_civil            |  Estado civil do cliente                                     | Object| Categórica         |\n",
    "| tipo_residencia         |  Tipo de residência                                      | Object| Categórica         |\n",
    "| idade                   |  Idade do cliente                                      | Inteiro| Discreta         |\n",
    "| tempo_emprego           |  Tempo de emprego (anos)                                     | Float| Contínua         |\n",
    "| qt_pessoas_residencia   |  Número de pessoas na residência                                      | Inteiro| Discreta         |\n",
    "| renda                   |  Renda mensal do cliente                                  | Float| Contínua         |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Vizualização da distribuição das observações das features contínuas , com kernel density estimate (KDE) plot.\n",
    "\n",
    "df_filtrado = df[df['renda'] <= 25000]\n",
    "plt.figure(figsize=(24, 6))\n",
    "sns.histplot(df_filtrado['renda'], kde=True)\n",
    "plt.title(f'Distribuição das observações da renda', fontsize=14)\n",
    "plt.xlabel('Renda')\n",
    "plt.ylabel('Frequência')\n",
    "plt.xticks(range(0, 25000, 1000))\n",
    "plt.show()\n",
    "\n",
    "# 2. Pair Plot\n",
    "\n",
    "sns.pairplot(df.select_dtypes(include=['float', 'int']))\n",
    "plt.suptitle(\"Pair Plot das features numéricas\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3 Crisp-DM: Preparação dos dados\n",
    "Nessa etapa realizamos tipicamente as seguintes operações com os dados:\n",
    "\n",
    " - **seleção**: Já temos os dados selecionados adequadamente?\n",
    " - **limpeza**: Precisaremos identificar e tratar dados faltantes\n",
    " - **construção**: construção de novas variáveis\n",
    " - **integração**: Temos apenas uma fonte de dados, não é necessário integração\n",
    " - **formatação**: Os dados já se encontram em formatos úteis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação dos Dados\n",
    "\n",
    "A preparação dos dados foi realizada utilizando o `ColumnTransformer` da biblioteca `scikit-learn`, permitindo pipelines separados para variáveis numéricas e categóricas. As etapas principais foram:\n",
    "\n",
    "### 1. Seleção de Variáveis\n",
    "\n",
    "Foram removidas colunas não informativas ou que poderiam causar vazamento de dados:\n",
    "\n",
    "- `Unnamed: 0`, `id_cliente` e `data_ref`: colunas de identificação ou temporais sem pré-processamento adequado.\n",
    "- `renda`: variável alvo, separada das features para modelagem supervisionada.\n",
    "\n",
    "### 2. Tratamento de Valores Ausentes\n",
    "\n",
    "- **Variáveis numéricas** (`idade`, `tempo_emprego`, `qt_pessoas_residencia`, `qtd_filhos`) foram imputadas com a **mediana**, estratégia robusta a outliers que mantém a distribuição central dos dados.\n",
    "- **Variáveis categóricas** (`sexo`, `posse_de_veiculo`, `posse_de_imovel`, `tipo_renda`, `educacao`, `estado_civil`, `tipo_residencia`) foram preenchidas com o valor constante `'missing'`. Isso evita a exclusão de registros e permite que o modelo capture padrões associados à ausência de informação.\n",
    "\n",
    "### 3. Escalonamento das Variáveis Numéricas\n",
    "\n",
    "As variáveis numéricas foram padronizadas com `StandardScaler`, transformando a média para 0 e o desvio padrão para 1. Embora não essencial para modelos baseados em árvore (como o LightGBM), a padronização pode:\n",
    "\n",
    "- Facilitar o entendimento da importância das variáveis.\n",
    "- Aumentar a compatibilidade com outros algoritmos sensíveis à escala.\n",
    "- Tornar o pipeline mais reutilizável em contextos distintos.\n",
    "\n",
    "### 4. Codificação de Variáveis Categóricas\n",
    "\n",
    "As variáveis categóricas foram transformadas com `OneHotEncoder`, utilizando o parâmetro `drop='first'` para evitar a multicolinearidade (armadilha das variáveis fictícias). Isso resulta em uma representação binária das categorias, preservando informação sem redundância linear.\n",
    "\n",
    "### 5. Transformação da Variável Alvo\n",
    "\n",
    "A variável `renda` foi transformada usando `np.log1p`, ou seja:\n",
    "\n",
    "\\[\n",
    "y = \\log(1 + \\text{renda})\n",
    "\\]\n",
    "\n",
    "Essa transformação é recomendada para variáveis com distribuição altamente assimétrica, como renda, e tem os seguintes benefícios:\n",
    "\n",
    "- Reduz a heterocedasticidade.\n",
    "- Atenua o impacto de outliers.\n",
    "- Melhora a capacidade preditiva do modelo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "import shap\n",
    "\n",
    "# Carregando os dados\n",
    "df = pd.read_csv(\"./test/input/previsao_de_renda.csv\")\n",
    "\n",
    "# Removendo colunas não úteis\n",
    "X = df.drop(columns=['Unnamed: 0', 'id_cliente', 'data_ref', 'renda'])\n",
    "y = df['renda']\n",
    "\n",
    "# Transformação logarítmica na variável target\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Separando tipos de colunas\n",
    "numeric_features = ['idade', 'tempo_emprego', 'qt_pessoas_residencia', 'qtd_filhos']\n",
    "categorical_features = ['sexo', 'posse_de_veiculo', 'posse_de_imovel', 'tipo_renda', 'educacao', 'estado_civil', 'tipo_residencia']\n",
    "\n",
    "# Define imputers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "# Pré-processador\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "numeric_feature_names = numeric_features\n",
    "\n",
    "# Pipeline de modelagem\n",
    "model = LGBMRegressor(n_estimators=2000, learning_rate=0.02, subsample=0.8, random_state=42)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Divisão dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinamento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "numeric_feature_names = numeric_features\n",
    "preprocessor2 = pipeline.named_steps['preprocessor']\n",
    "numeric_feature_names = numeric_features\n",
    "\n",
    "# Obtendo as categorical features depois da transformação\n",
    "ohe = preprocessor2.named_transformers_['cat'].named_steps['onehot']\n",
    "categorical_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combinar as feature names\n",
    "all_feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "\n",
    "# Obter os feature names\n",
    "ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "categorical_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "all_feature_names = numeric_features + list(categorical_feature_names)\n",
    "\n",
    "# Computar SHAP values\n",
    "explainer = shap.Explainer(pipeline.named_steps['model'])\n",
    "X_transformed = preprocessor.transform(X_test)\n",
    "shap_values = explainer(X_transformed)\n",
    "\n",
    "# Inject correct feature names into the SHAP Explanation object\n",
    "shap_values.feature_names = all_feature_names\n",
    "\n",
    "# Plot\n",
    "shap.plots.beeswarm(shap_values, max_display=15)\n",
    "\n",
    "# Predição e avaliação\n",
    "y_pred_log = pipeline.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Visualização da distribuição dos resíduos\n",
    "residuals = y_true - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True)\n",
    "plt.title(\"Distribuição dos Resíduos\")\n",
    "plt.xlabel(\"Erro de Previsão (R$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4 Crisp-DM: Modelagem\n",
    "## Modelagem\n",
    "\n",
    "A modelagem foi realizada utilizando o algoritmo `LightGBM Regressor`, um método de **gradient boosting baseado em histogramas**, altamente eficiente para tarefas de regressão com dados tabulares. \n",
    "\n",
    "A variável alvo `renda` foi previamente transformada com `log1p` (`log(1 + renda)`) com o objetivo de:\n",
    "\n",
    "- Reduzir a **heterocedasticidade**,\n",
    "- Minimizar o impacto de **outliers**,\n",
    "- Melhorar a performance preditiva do modelo ao lidar com uma variável com distribuição altamente assimétrica (comum em dados financeiros).\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline de Modelagem\n",
    "\n",
    "Foi utilizado o `sklearn.Pipeline` para encadear todas as etapas de pré-processamento e modelagem, promovendo **reprodutibilidade**, **modularidade** e facilitando o deployment. O pipeline incluiu:\n",
    "\n",
    "#### 🔧 Imputação de Valores Ausentes\n",
    "- **Numéricas**: preenchidas com a **mediana**, estratégia robusta a valores extremos.\n",
    "- **Categóricas**: preenchidas com o valor constante `'missing'`, preservando informações relevantes da ausência.\n",
    "\n",
    "#### 🔢 Normalização\n",
    "- Variáveis numéricas foram escaladas com `StandardScaler`, padronizando média 0 e desvio padrão 1.\n",
    "- Embora modelos baseados em árvore não exijam normalização, isso melhora a consistência do pipeline e compatibilidade com outras técnicas.\n",
    "\n",
    "#### 🧬 Codificação\n",
    "- Variáveis categóricas foram codificadas com `OneHotEncoder` com `drop='first'`, eliminando uma categoria por variável para evitar **colinearidade** com o intercepto.\n",
    "\n",
    "---\n",
    "\n",
    "### Hiperparâmetros do Modelo\n",
    "\n",
    "O `LightGBM Regressor` foi configurado com os seguintes hiperparâmetros:\n",
    "\n",
    "- `n_estimators=1000`: número máximo de árvores no ensemble.\n",
    "- `learning_rate=0.05`: controla o passo de atualização em cada iteração, permitindo treinamento mais estável.\n",
    "- `subsample=0.8`: ativa o **stochastic boosting**, usando apenas 80% dos dados em cada árvore para melhorar a generalização.\n",
    "- `random_state=42`: garante **reprodutibilidade** dos resultados.\n",
    "\n",
    "---\n",
    "\n",
    "### Pós-processamento das Predições\n",
    "\n",
    "Após o treinamento, as previsões foram convertidas de volta da escala logarítmica com a função `np.expm1`, revertendo a transformação `log1p` e permitindo interpretação direta dos valores preditos de **renda** em reais.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5 Crisp-DM: Avaliação dos resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de treinamento foi realizado utilizando o LightGBM, uma estrutura de gradient boosting altamente otimizada para grandes volumes de dados. O conjunto de treinamento consistiu em 12.000 pontos de dados e 22 características. Com base nas informações iniciais, o modelo utiliza multi-threading por linha, o que é automaticamente selecionado pelo LightGBM com base no custo da análise de sobrecarga (0.000384 segundos). Esse processo reduz significativamente o tempo de treinamento em comparação com outros métodos, mas vale destacar que o tempo de sobrecarga é negligível e não afeta o desempenho geral.\n",
    "\n",
    "O score inicial de treinamento do modelo foi 8.202201, o que pode indicar o valor inicial de previsão antes da otimização. Um ponto importante é o valor de Total Bins, que é 356. Isso representa o número de intervalos discretizados que o modelo usa para dividir características contínuas. Essa abordagem é crucial para a eficiência do LightGBM, pois permite decisões mais rápidas e precisas com base na estrutura dos dados.\n",
    "\n",
    "Métricas de Avaliação:\n",
    "Erro Absoluto Médio (MAE): O MAE foi de 2775,40, o que indica que, em média, as previsões do modelo se desviam dos valores reais em cerca de 2775. Essa métrica fornece uma noção intuitiva da magnitude dos erros nas previsões do modelo, e, dado a escala dos dados, esse valor pode ser aceitável ou não, dependendo do contexto de negócio e da faixa de valores da variável alvo.\n",
    "\n",
    "Erro Quadrático Médio (RMSE): O RMSE foi de 5419,87, o que fornece uma medida mais sensível para erros maiores, penalizando previsões com maiores desvios de forma mais acentuada do que o MAE. O RMSE é significativamente mais alto que o MAE, o que sugere que existem alguns outliers (valores extremos) nos dados que o modelo está tendo dificuldades em prever corretamente.\n",
    "\n",
    "Análises e Próximos Passos:\n",
    "Desempenho do Modelo: A diferença considerável entre o MAE e o RMSE indica a presença de possíveis outliers ou uma distribuição de erros assimétrica. Isso sugere que, enquanto o modelo pode estar fazendo previsões razoáveis para a maioria dos dados, ele está sendo penalizado desproporcionalmente por alguns erros maiores. Esse ponto pode indicar a necessidade de uma análise mais profunda dos resíduos para identificar e tratar outliers ou anomalias nos dados.\n",
    "\n",
    "Ajuste de Hiperparâmetros: Dada a diferença significativa entre o MAE e o RMSE, pode ser interessante realizar uma otimização de hiperparâmetros (usando técnicas como Random Search, Grid Search ou Bayesian Optimization) para tentar reduzir as tendências de overfitting ou underfitting do modelo. Ajustar parâmetros como num_leaves, learning_rate ou max_depth pode ajudar a alcançar um equilíbrio melhor entre viés e variância.\n",
    "\n",
    "Engenharia de Características: O modelo utiliza 22 características, e seria interessante verificar se alguma interação entre características ou transformações (por exemplo, logaritmos, polinômios ou discretização de variáveis) poderia melhorar o desempenho, especialmente em relação aos outliers.\n",
    "\n",
    "Regularização: Dado o RMSE relativamente alto, aplicar métodos de regularização, como L1/L2, pode ajudar a suavizar as previsões, reduzindo a influência de outliers e melhorando a capacidade de generalização do modelo.\n",
    "\n",
    "Avaliação: Avaliar o modelo por meio de outras métricas, como o R² (coeficiente de determinação), também pode fornecer uma visão mais detalhada sobre a proporção da variabilidade explicada pelo modelo. Além disso, a validação cruzada em diferentes subconjuntos dos dados pode ajudar a avaliar a robustez do modelo e prevenir o overfitting.\n",
    "\n",
    "Em resumo, embora o modelo apresente um desempenho razoável com o LightGBM, as métricas de erro observadas sugerem que há espaço para melhorias, especialmente em relação aos outliers. Uma combinação de ajuste de hiperparâmetros, engenharia de características e maior regularização pode melhorar a precisão preditiva e a capacidade de generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6 Crisp-DM: Implantação\n",
    "Nessa etapa colocamos em uso o modelo desenvolvido, normalmente implementando o modelo desenvolvido em um motor que toma as decisões com algum nível de automação."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
