{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previs√£o de renda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 elementos importantes\n",
    "- Esse notebook\n",
    "- Streamlit com as an√°lises\n",
    "- Seu Github com o projeto\n",
    "- V√≠deo no readme do github mostrando o streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 1 CRISP - DM: Entendimento do neg√≥cio\n",
    "\n",
    "Os bancos s√£o institui√ß√µes financeiras respons√°veis por oferecer diversos servi√ßos, como contas correntes, investimentos, empr√©stimos e financiamentos. Para operar de maneira eficiente e segura, eles precisam avaliar o perfil financeiro de seus clientes, garantindo que possam oferecer produtos adequados sem comprometer sua sustentabilidade.  \n",
    "Um dos principais desafios enfrentados pelos bancos √© a avalia√ß√£o da renda dos clientes. Essa informa√ß√£o √© essencial para diversas opera√ß√µes, tais como:\n",
    "\n",
    "- **Concess√£o de cr√©dito**: Antes de aprovar um empr√©stimo ou financiamento, o banco precisa estimar a capacidade de pagamento do cliente para evitar inadimpl√™ncia.\n",
    "- **Defini√ß√£o de limites de cart√µes e cheque especial**: Bancos ajustam limites de cr√©dito com base na renda estimada do cliente.\n",
    "- **Personaliza√ß√£o de produtos financeiros**: Com uma previs√£o precisa de renda, √© poss√≠vel oferecer investimentos e servi√ßos mais alinhados ao perfil do cliente.\n",
    "- **Compliance e regulamenta√ß√£o**: Autoridades regulat√≥rias exigem que os bancos adotem pr√°ticas respons√°veis na concess√£o de cr√©dito, o que exige an√°lises detalhadas da situa√ß√£o financeira dos clientes.\n",
    "- **Personaliza√ß√£o de investimentos**: Com a previs√£o de renda, √© poss√≠vel oferecer op√ß√µes de investimentos adequadas ao perfil financeiro do cliente, como fundos de baixo risco para quem tem uma renda mais restrita, ou alternativas de maior retorno para clientes com maior capacidade financeira.\n",
    "\n",
    "No entanto, nem sempre a informa√ß√£o de renda est√° dispon√≠vel ou √© declarada corretamente pelos clientes. Para contornar essa limita√ß√£o, os bancos podem utilizar modelos de Machine Learning para prever a renda de um cliente com base em outras vari√°veis do seu perfil, como idade, hist√≥rico de cr√©dito, comportamento financeiro e dados demogr√°ficos.\n",
    "\n",
    "Neste projeto, o objetivo √© desenvolver um modelo preditivo para estimar a renda dos clientes com base em seus atributos, permitindo que o banco otimize sua tomada de decis√£o e melhore a experi√™ncia do usu√°rio fornecendo uma ferramenta que ajudar√° o cliente a tomar cr√©dito de forma correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 2 Crisp-DM: Entendimento dos dados\n",
    "\n",
    "<span >Na etapa de Compreens√£o dos Dados do CRISP-DM, o foco est√° na coleta, an√°lise e explora√ß√£o inicial dos dados dispon√≠veis. O objetivo principal √© entender a estrutura, qualidade e relev√¢ncia dos dados para o projeto. Aqui est√£o os passos detalhados para essa etapa, utilizando as bibliotecas mencionadas.\n",
    "\n",
    "1. Coleta e Identifica√ß√£o das Fontes de Dados: A coleta de dados pode ocorrer de diversas fontes, como SQL, Excel, APIs externas, entre outros. Os dados podem ser coletados em intervalos di√°rios, mensais ou anuais, dependendo da necessidade do neg√≥cio. Alguns dados podem vir de servidores internos da empresa, enquanto outros podem ser coletados externamente, como taxas de infla√ß√£o calculadas por fontes oficiais ou dados de fornecedores como SERASA ou Boa Vista. Dados de crawlers da web tamb√©m podem ser utilizados para enriquecer a base de dados. Neste caso o csv foi fornecido. E para sua manipula√ß√£o utilizaremos o Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 15 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             15000 non-null  int64  \n",
      " 1   data_ref               15000 non-null  object \n",
      " 2   id_cliente             15000 non-null  int64  \n",
      " 3   sexo                   15000 non-null  object \n",
      " 4   posse_de_veiculo       15000 non-null  bool   \n",
      " 5   posse_de_imovel        15000 non-null  bool   \n",
      " 6   qtd_filhos             15000 non-null  int64  \n",
      " 7   tipo_renda             15000 non-null  object \n",
      " 8   educacao               15000 non-null  object \n",
      " 9   estado_civil           15000 non-null  object \n",
      " 10  tipo_residencia        15000 non-null  object \n",
      " 11  idade                  15000 non-null  int64  \n",
      " 12  tempo_emprego          12427 non-null  float64\n",
      " 13  qt_pessoas_residencia  15000 non-null  float64\n",
      " 14  renda                  15000 non-null  float64\n",
      "dtypes: bool(2), float64(3), int64(4), object(6)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"./test/input/previsao_de_renda.csv\")  # Exemplo de carregamento de dados\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removeremos as colunas: id_cliente, Unnamed: 0 e data_ref pois n√£o fazem sentido na an√°lise neste momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sexo                   15000 non-null  object \n",
      " 1   posse_de_veiculo       15000 non-null  bool   \n",
      " 2   posse_de_imovel        15000 non-null  bool   \n",
      " 3   qtd_filhos             15000 non-null  int64  \n",
      " 4   tipo_renda             15000 non-null  object \n",
      " 5   educacao               15000 non-null  object \n",
      " 6   estado_civil           15000 non-null  object \n",
      " 7   tipo_residencia        15000 non-null  object \n",
      " 8   idade                  15000 non-null  int64  \n",
      " 9   tempo_emprego          12427 non-null  float64\n",
      " 10  qt_pessoas_residencia  15000 non-null  int32  \n",
      " 11  renda                  15000 non-null  float64\n",
      "dtypes: bool(2), float64(2), int32(1), int64(2), object(5)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df['qt_pessoas_residencia'] = df['qt_pessoas_residencia'].astype(int)\n",
    "df.drop(columns=['id_cliente','Unnamed: 0','data_ref'], inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. An√°lise Explorat√≥ria Inicial: Com os dados coletados, a primeira a√ß√£o √© carreg√°-los no ambiente de trabalho para uma an√°lise preliminar. Usando pandas, pode-se importar os dados de diferentes formatos como CSV, Excel, ou bancos de dados SQL. O primeiro passo √© realizar uma inspe√ß√£o dos dados para identificar as vari√°veis dispon√≠veis, seus tipos, e verificar a presen√ßa de valores ausentes ou inconsistentes. Para isso utilizaremos **ProfileReport** do **ydata_profiling**, O ydata-profiling √© uma ferramenta de ponta na etapa de entendimento dos dados no fluxo de trabalho de ci√™ncia de dados, sendo um pacote pioneiro em Python para perfilagem de dados. O **ydata_profiling** √© um pacote amplamente utilizado para perfilagem autom√°tica de dados, padronizando a gera√ß√£o de relat√≥rios detalhados que incluem estat√≠sticas descritivas e visualiza√ß√µes, facilitando a an√°lise explorat√≥ria e a identifica√ß√£o de padr√µes, anomalias e correla√ß√µes nas vari√°veis. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import warnings\n",
    "# Gerar relat√≥rio sem autocorrela√ß√£o\n",
    "profile = ProfileReport(\n",
    "    df,\n",
    "    explorative=True,\n",
    "    correlations={\"auto\": {\"calculate\": False}}  # <- isso desativa a autocorrela√ß√£o\n",
    ")\n",
    "\n",
    "# Save the report as an HTML file\n",
    "profile.to_file(\"report.html\")\n",
    "\n",
    "# Or display in a Jupyter Notebook\n",
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProfileReport revelou destaques do dataset atrav√©s do seu Overview e seu Alerts:\n",
    "- **Vari√°veis**: Total de 12. 5 Categ√≥ricas,  2 Booleanas, 5 Num√©ricas.Tamb√©m revelou 15000 observa√ß√µes de cada com 3460 duplicadas, o que pode ser maior considerendo todas as colunas do dataset.\n",
    "- **qtd_filhos**: tem 944 (6.3%) zeros.\n",
    "- **tempo_emprego**: tem 2573 (17.2%) valores faltantes.\n",
    "- **tipo_residencia**: tem um alto imbalance (75.1%).\n",
    "- **Consumo geral de mem√≥ria**: 5,7mb, com tamanho m√©dio de linha salva de 396,7 bytes.\n",
    "\n",
    "Estes resultados sugerem quais tipos de tratamento o dataset precisa para atingir a qualidade necess√°ria para as pr√≥ximas etapas do CRISP-DM como:\n",
    "- **Tratamento de Nan/Missing Values**\n",
    "- **Tratamento de Linhas Duplicadas**\n",
    "- **Imbalance**\n",
    "- **Normaliza√ß√£o**\n",
    "- **Dummies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dicion√°rio de dados\n",
    "\n",
    "| Vari√°vel                | Descri√ß√£o                                           | Tipo         | Natureza         |\n",
    "| ----------------------- |:---------------------------------------------------:| ------------:| ----------------:|\n",
    "| data_ref                |  Data da refer√™ncia da an√°lise                                      | Data| Temporal         |\n",
    "| id_cliente              |  Identifica√ß√£o √∫nica do cliente                                      | Inteiro| Discreta         |\n",
    "| sexo                    |  Sexo do cliente                                      | Object| Categ√≥rica         |\n",
    "| posse_de_veiculo        |  Se o cliente possui ve√≠culo                                      | Booleano| Categ√≥rica         |\n",
    "| posse_de_imovel         |  Se o cliente possui im√≥vel                                     | Booleano| Categ√≥rica         |\n",
    "| qtd_filhos              |  N√∫mero de filhos                                     | Inteiro| Discreta         |\n",
    "| tipo_renda              |  Tipo de fonte de renda                                    | Object| Categ√≥rica         |\n",
    "| educacao                |  N√≠vel de educa√ß√£o                                     | Object| Categ√≥rica         |\n",
    "| estado_civil            |  Estado civil do cliente                                     | Object| Categ√≥rica         |\n",
    "| tipo_residencia         |  Tipo de resid√™ncia                                      | Object| Categ√≥rica         |\n",
    "| idade                   |  Idade do cliente                                      | Inteiro| Discreta         |\n",
    "| tempo_emprego           |  Tempo de emprego (anos)                                     | Float| Cont√≠nua         |\n",
    "| qt_pessoas_residencia   |  N√∫mero de pessoas na resid√™ncia                                      | Inteiro| Discreta         |\n",
    "| renda                   |  Renda mensal do cliente                                  | Float| Cont√≠nua         |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Vizualiza√ß√£o da distribui√ß√£o das observa√ß√µes das features cont√≠nuas , com kernel density estimate (KDE) plot.\n",
    "\n",
    "df_filtrado = df[df['renda'] <= 25000]\n",
    "plt.figure(figsize=(24, 6))\n",
    "sns.histplot(df_filtrado['renda'], kde=True)\n",
    "plt.title(f'Distribui√ß√£o das observa√ß√µes da renda', fontsize=14)\n",
    "plt.xlabel('Renda')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.xticks(range(0, 25000, 1000))\n",
    "plt.show()\n",
    "\n",
    "# 2. Pair Plot\n",
    "\n",
    "sns.pairplot(df.select_dtypes(include=['float', 'int']))\n",
    "plt.suptitle(\"Pair Plot das features num√©ricas\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 3 Crisp-DM: Prepara√ß√£o dos dados\n",
    "Nessa etapa realizamos tipicamente as seguintes opera√ß√µes com os dados:\n",
    "\n",
    " - **sele√ß√£o**: J√° temos os dados selecionados adequadamente?\n",
    " - **limpeza**: Precisaremos identificar e tratar dados faltantes\n",
    " - **constru√ß√£o**: constru√ß√£o de novas vari√°veis\n",
    " - **integra√ß√£o**: Temos apenas uma fonte de dados, n√£o √© necess√°rio integra√ß√£o\n",
    " - **formata√ß√£o**: Os dados j√° se encontram em formatos √∫teis?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepara√ß√£o dos Dados\n",
    "\n",
    "A prepara√ß√£o dos dados foi realizada utilizando o `ColumnTransformer` da biblioteca `scikit-learn`, permitindo pipelines separados para vari√°veis num√©ricas e categ√≥ricas. As etapas principais foram:\n",
    "\n",
    "### 1. Sele√ß√£o de Vari√°veis\n",
    "\n",
    "Foram removidas colunas n√£o informativas ou que poderiam causar vazamento de dados:\n",
    "\n",
    "- `Unnamed: 0`, `id_cliente` e `data_ref`: colunas de identifica√ß√£o ou temporais sem pr√©-processamento adequado.\n",
    "- `renda`: vari√°vel alvo, separada das features para modelagem supervisionada.\n",
    "\n",
    "### 2. Tratamento de Valores Ausentes\n",
    "\n",
    "- **Vari√°veis num√©ricas** (`idade`, `tempo_emprego`, `qt_pessoas_residencia`, `qtd_filhos`) foram imputadas com a **mediana**, estrat√©gia robusta a outliers que mant√©m a distribui√ß√£o central dos dados.\n",
    "- **Vari√°veis categ√≥ricas** (`sexo`, `posse_de_veiculo`, `posse_de_imovel`, `tipo_renda`, `educacao`, `estado_civil`, `tipo_residencia`) foram preenchidas com o valor constante `'missing'`. Isso evita a exclus√£o de registros e permite que o modelo capture padr√µes associados √† aus√™ncia de informa√ß√£o.\n",
    "\n",
    "### 3. Escalonamento das Vari√°veis Num√©ricas\n",
    "\n",
    "As vari√°veis num√©ricas foram padronizadas com `StandardScaler`, transformando a m√©dia para 0 e o desvio padr√£o para 1. Embora n√£o essencial para modelos baseados em √°rvore (como o LightGBM), a padroniza√ß√£o pode:\n",
    "\n",
    "- Facilitar o entendimento da import√¢ncia das vari√°veis.\n",
    "- Aumentar a compatibilidade com outros algoritmos sens√≠veis √† escala.\n",
    "- Tornar o pipeline mais reutiliz√°vel em contextos distintos.\n",
    "\n",
    "### 4. Codifica√ß√£o de Vari√°veis Categ√≥ricas\n",
    "\n",
    "As vari√°veis categ√≥ricas foram transformadas com `OneHotEncoder`, utilizando o par√¢metro `drop='first'` para evitar a multicolinearidade (armadilha das vari√°veis fict√≠cias). Isso resulta em uma representa√ß√£o bin√°ria das categorias, preservando informa√ß√£o sem redund√¢ncia linear.\n",
    "\n",
    "### 5. Transforma√ß√£o da Vari√°vel Alvo\n",
    "\n",
    "A vari√°vel `renda` foi transformada usando `np.log1p`, ou seja:\n",
    "\n",
    "\\[\n",
    "y = \\log(1 + \\text{renda})\n",
    "\\]\n",
    "\n",
    "Essa transforma√ß√£o √© recomendada para vari√°veis com distribui√ß√£o altamente assim√©trica, como renda, e tem os seguintes benef√≠cios:\n",
    "\n",
    "- Reduz a heterocedasticidade.\n",
    "- Atenua o impacto de outliers.\n",
    "- Melhora a capacidade preditiva do modelo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "import shap\n",
    "\n",
    "# Carregando os dados\n",
    "df = pd.read_csv(\"./test/input/previsao_de_renda.csv\")\n",
    "\n",
    "# Removendo colunas n√£o √∫teis\n",
    "X = df.drop(columns=['Unnamed: 0', 'id_cliente', 'data_ref', 'renda'])\n",
    "y = df['renda']\n",
    "\n",
    "# Transforma√ß√£o logar√≠tmica na vari√°vel target\n",
    "y_log = np.log1p(y)\n",
    "\n",
    "# Separando tipos de colunas\n",
    "numeric_features = ['idade', 'tempo_emprego', 'qt_pessoas_residencia', 'qtd_filhos']\n",
    "categorical_features = ['sexo', 'posse_de_veiculo', 'posse_de_imovel', 'tipo_renda', 'educacao', 'estado_civil', 'tipo_residencia']\n",
    "\n",
    "# Define imputers\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(drop='first'))\n",
    "])\n",
    "\n",
    "# Pr√©-processador\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "numeric_feature_names = numeric_features\n",
    "\n",
    "# Pipeline de modelagem\n",
    "model = LGBMRegressor(n_estimators=2000, learning_rate=0.02, subsample=0.8, random_state=42)\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Divis√£o dos dados\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinamento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "numeric_feature_names = numeric_features\n",
    "preprocessor2 = pipeline.named_steps['preprocessor']\n",
    "numeric_feature_names = numeric_features\n",
    "\n",
    "# Obtendo as categorical features depois da transforma√ß√£o\n",
    "ohe = preprocessor2.named_transformers_['cat'].named_steps['onehot']\n",
    "categorical_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combinar as feature names\n",
    "all_feature_names = list(numeric_feature_names) + list(categorical_feature_names)\n",
    "\n",
    "# Obter os feature names\n",
    "ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "categorical_feature_names = ohe.get_feature_names_out(categorical_features)\n",
    "all_feature_names = numeric_features + list(categorical_feature_names)\n",
    "\n",
    "# Computar SHAP values\n",
    "explainer = shap.Explainer(pipeline.named_steps['model'])\n",
    "X_transformed = preprocessor.transform(X_test)\n",
    "shap_values = explainer(X_transformed)\n",
    "\n",
    "# Inject correct feature names into the SHAP Explanation object\n",
    "shap_values.feature_names = all_feature_names\n",
    "\n",
    "# Plot\n",
    "shap.plots.beeswarm(shap_values, max_display=15)\n",
    "\n",
    "# Predi√ß√£o e avalia√ß√£o\n",
    "y_pred_log = pipeline.predict(X_test)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "y_true = np.expm1(y_test)\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Visualiza√ß√£o da distribui√ß√£o dos res√≠duos\n",
    "residuals = y_true - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, bins=50, kde=True)\n",
    "plt.title(\"Distribui√ß√£o dos Res√≠duos\")\n",
    "plt.xlabel(\"Erro de Previs√£o (R$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4 Crisp-DM: Modelagem\n",
    "## Modelagem\n",
    "\n",
    "A modelagem foi realizada utilizando o algoritmo `LightGBM Regressor`, um m√©todo de **gradient boosting baseado em histogramas**, altamente eficiente para tarefas de regress√£o com dados tabulares. \n",
    "\n",
    "A vari√°vel alvo `renda` foi previamente transformada com `log1p` (`log(1 + renda)`) com o objetivo de:\n",
    "\n",
    "- Reduzir a **heterocedasticidade**,\n",
    "- Minimizar o impacto de **outliers**,\n",
    "- Melhorar a performance preditiva do modelo ao lidar com uma vari√°vel com distribui√ß√£o altamente assim√©trica (comum em dados financeiros).\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline de Modelagem\n",
    "\n",
    "Foi utilizado o `sklearn.Pipeline` para encadear todas as etapas de pr√©-processamento e modelagem, promovendo **reprodutibilidade**, **modularidade** e facilitando o deployment. O pipeline incluiu:\n",
    "\n",
    "#### üîß Imputa√ß√£o de Valores Ausentes\n",
    "- **Num√©ricas**: preenchidas com a **mediana**, estrat√©gia robusta a valores extremos.\n",
    "- **Categ√≥ricas**: preenchidas com o valor constante `'missing'`, preservando informa√ß√µes relevantes da aus√™ncia.\n",
    "\n",
    "#### üî¢ Normaliza√ß√£o\n",
    "- Vari√°veis num√©ricas foram escaladas com `StandardScaler`, padronizando m√©dia 0 e desvio padr√£o 1.\n",
    "- Embora modelos baseados em √°rvore n√£o exijam normaliza√ß√£o, isso melhora a consist√™ncia do pipeline e compatibilidade com outras t√©cnicas.\n",
    "\n",
    "#### üß¨ Codifica√ß√£o\n",
    "- Vari√°veis categ√≥ricas foram codificadas com `OneHotEncoder` com `drop='first'`, eliminando uma categoria por vari√°vel para evitar **colinearidade** com o intercepto.\n",
    "\n",
    "---\n",
    "\n",
    "### Hiperpar√¢metros do Modelo\n",
    "\n",
    "O `LightGBM Regressor` foi configurado com os seguintes hiperpar√¢metros:\n",
    "\n",
    "- `n_estimators=1000`: n√∫mero m√°ximo de √°rvores no ensemble.\n",
    "- `learning_rate=0.05`: controla o passo de atualiza√ß√£o em cada itera√ß√£o, permitindo treinamento mais est√°vel.\n",
    "- `subsample=0.8`: ativa o **stochastic boosting**, usando apenas 80% dos dados em cada √°rvore para melhorar a generaliza√ß√£o.\n",
    "- `random_state=42`: garante **reprodutibilidade** dos resultados.\n",
    "\n",
    "---\n",
    "\n",
    "### P√≥s-processamento das Predi√ß√µes\n",
    "\n",
    "Ap√≥s o treinamento, as previs√µes foram convertidas de volta da escala logar√≠tmica com a fun√ß√£o `np.expm1`, revertendo a transforma√ß√£o `log1p` e permitindo interpreta√ß√£o direta dos valores preditos de **renda** em reais.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 5 Crisp-DM: Avalia√ß√£o dos resultados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O processo de treinamento foi realizado utilizando o LightGBM, uma estrutura de gradient boosting altamente otimizada para grandes volumes de dados. O conjunto de treinamento consistiu em 12.000 pontos de dados e 22 caracter√≠sticas. Com base nas informa√ß√µes iniciais, o modelo utiliza multi-threading por linha, o que √© automaticamente selecionado pelo LightGBM com base no custo da an√°lise de sobrecarga (0.000384 segundos). Esse processo reduz significativamente o tempo de treinamento em compara√ß√£o com outros m√©todos, mas vale destacar que o tempo de sobrecarga √© neglig√≠vel e n√£o afeta o desempenho geral.\n",
    "\n",
    "O score inicial de treinamento do modelo foi 8.202201, o que pode indicar o valor inicial de previs√£o antes da otimiza√ß√£o. Um ponto importante √© o valor de Total Bins, que √© 356. Isso representa o n√∫mero de intervalos discretizados que o modelo usa para dividir caracter√≠sticas cont√≠nuas. Essa abordagem √© crucial para a efici√™ncia do LightGBM, pois permite decis√µes mais r√°pidas e precisas com base na estrutura dos dados.\n",
    "\n",
    "M√©tricas de Avalia√ß√£o:\n",
    "Erro Absoluto M√©dio (MAE): O MAE foi de 2775,40, o que indica que, em m√©dia, as previs√µes do modelo se desviam dos valores reais em cerca de 2775. Essa m√©trica fornece uma no√ß√£o intuitiva da magnitude dos erros nas previs√µes do modelo, e, dado a escala dos dados, esse valor pode ser aceit√°vel ou n√£o, dependendo do contexto de neg√≥cio e da faixa de valores da vari√°vel alvo.\n",
    "\n",
    "Erro Quadr√°tico M√©dio (RMSE): O RMSE foi de 5419,87, o que fornece uma medida mais sens√≠vel para erros maiores, penalizando previs√µes com maiores desvios de forma mais acentuada do que o MAE. O RMSE √© significativamente mais alto que o MAE, o que sugere que existem alguns outliers (valores extremos) nos dados que o modelo est√° tendo dificuldades em prever corretamente.\n",
    "\n",
    "An√°lises e Pr√≥ximos Passos:\n",
    "Desempenho do Modelo: A diferen√ßa consider√°vel entre o MAE e o RMSE indica a presen√ßa de poss√≠veis outliers ou uma distribui√ß√£o de erros assim√©trica. Isso sugere que, enquanto o modelo pode estar fazendo previs√µes razo√°veis para a maioria dos dados, ele est√° sendo penalizado desproporcionalmente por alguns erros maiores. Esse ponto pode indicar a necessidade de uma an√°lise mais profunda dos res√≠duos para identificar e tratar outliers ou anomalias nos dados.\n",
    "\n",
    "Ajuste de Hiperpar√¢metros: Dada a diferen√ßa significativa entre o MAE e o RMSE, pode ser interessante realizar uma otimiza√ß√£o de hiperpar√¢metros (usando t√©cnicas como Random Search, Grid Search ou Bayesian Optimization) para tentar reduzir as tend√™ncias de overfitting ou underfitting do modelo. Ajustar par√¢metros como num_leaves, learning_rate ou max_depth pode ajudar a alcan√ßar um equil√≠brio melhor entre vi√©s e vari√¢ncia.\n",
    "\n",
    "Engenharia de Caracter√≠sticas: O modelo utiliza 22 caracter√≠sticas, e seria interessante verificar se alguma intera√ß√£o entre caracter√≠sticas ou transforma√ß√µes (por exemplo, logaritmos, polin√¥mios ou discretiza√ß√£o de vari√°veis) poderia melhorar o desempenho, especialmente em rela√ß√£o aos outliers.\n",
    "\n",
    "Regulariza√ß√£o: Dado o RMSE relativamente alto, aplicar m√©todos de regulariza√ß√£o, como L1/L2, pode ajudar a suavizar as previs√µes, reduzindo a influ√™ncia de outliers e melhorando a capacidade de generaliza√ß√£o do modelo.\n",
    "\n",
    "Avalia√ß√£o: Avaliar o modelo por meio de outras m√©tricas, como o R¬≤ (coeficiente de determina√ß√£o), tamb√©m pode fornecer uma vis√£o mais detalhada sobre a propor√ß√£o da variabilidade explicada pelo modelo. Al√©m disso, a valida√ß√£o cruzada em diferentes subconjuntos dos dados pode ajudar a avaliar a robustez do modelo e prevenir o overfitting.\n",
    "\n",
    "Em resumo, embora o modelo apresente um desempenho razo√°vel com o LightGBM, as m√©tricas de erro observadas sugerem que h√° espa√ßo para melhorias, especialmente em rela√ß√£o aos outliers. Uma combina√ß√£o de ajuste de hiperpar√¢metros, engenharia de caracter√≠sticas e maior regulariza√ß√£o pode melhorar a precis√£o preditiva e a capacidade de generaliza√ß√£o do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 6 Crisp-DM: Implanta√ß√£o\n",
    "Nessa etapa colocamos em uso o modelo desenvolvido, normalmente implementando o modelo desenvolvido em um motor que toma as decis√µes com algum n√≠vel de automa√ß√£o."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
